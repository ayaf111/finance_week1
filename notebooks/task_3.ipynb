{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "folder_path = \"../yfinance_data/the file\"  # Make sure this path is correct\n",
    "\n",
    "# Dictionary to hold individual DataFrames\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        stock_name = filename.replace(\".csv\", \"\")\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load CSV into DataFrame with datetime index\n",
    "        df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "        \n",
    "        # Ensure index is datetime and add normalized date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df['Date'] = df.index.normalize()\n",
    "        \n",
    "        # Save to dictionary\n",
    "        data_dict[stock_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b4b89d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC-04:00]\n",
      "0   2020-06-05 10:30:54-04:00\n",
      "1   2020-06-03 10:45:20-04:00\n",
      "2   2020-05-26 04:30:07-04:00\n",
      "3   2020-05-22 12:45:06-04:00\n",
      "4   2020-05-22 11:38:59-04:00\n",
      "Name: date, dtype: datetime64[ns, UTC-04:00]\n"
     ]
    }
   ],
   "source": [
    "# For news data\n",
    "news_df=pd.read_csv(\"C:/Users/Student/Documents/kaim documents/week 1/raw_analyst_ratings.csv/raw_analyst_ratings.csv\")\n",
    "# Step 1: Convert to datetime (safely)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "print(news_df['date'].dtype)\n",
    "print(news_df['date'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78dcaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC-04:00]\n",
      "0   2020-06-05 10:30:54-04:00\n",
      "1   2020-06-03 10:45:20-04:00\n",
      "2   2020-05-26 04:30:07-04:00\n",
      "3   2020-05-22 12:45:06-04:00\n",
      "4   2020-05-22 11:38:59-04:00\n",
      "Name: date, dtype: datetime64[ns, UTC-04:00]\n",
      "1407266   2018-01-05\n",
      "1407267   2017-12-06\n",
      "1407268   2017-12-06\n",
      "1407269   2017-11-15\n",
      "1407270   2017-11-14\n",
      "Name: date, dtype: datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"C:/Users/Student/Documents/kaim documents/week 1/raw_analyst_ratings.csv/raw_analyst_ratings.csv\")\n",
    "\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "print(news_df['date'].dtype)\n",
    "print(news_df['date'].head())\n",
    "\n",
    "news_df = news_df.dropna(subset=['date'])  # Drop NaT values\n",
    "\n",
    "# Remove timezone to avoid errors with .dt.normalize()\n",
    "news_df['date'] = news_df['date'].dt.tz_localize(None)\n",
    "\n",
    "# Normalize date to midnight\n",
    "news_df['date'] = news_df['date'].dt.normalize()\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "print(news_df['date'].tail())\n",
    "print(news_df['date'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e92816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import time\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "58668f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your datasets\n",
    "def load_data():\n",
    "    \"\"\"Load news and stock data from your specific paths\"\"\"\n",
    "    # Load news data\n",
    "    news_df = pd.read_csv(\"C:/Users/Student/Documents/kaim documents/week 1/raw_analyst_ratings.csv/raw_analyst_ratings.csv\")\n",
    "    \n",
    "    # Load stock data from multiple files\n",
    "    folder_path = \"../yfinance_data/the file\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            stock_name = filename.replace(\".csv\", \"\")\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data_dict[stock_name] = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "    \n",
    "    return news_df, data_dict\n",
    "news_df, data_dict = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a5b20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2024-07-24  225.419998  225.990005  214.710007  215.990005  215.990005   \n",
      "2024-07-25  216.800003  226.000000  216.229996  220.250000  220.250000   \n",
      "2024-07-26  221.190002  222.279999  215.330002  219.800003  219.800003   \n",
      "2024-07-29  224.899994  234.270004  224.699997  232.100006  232.100006   \n",
      "2024-07-30  232.250000  232.410004  220.000000  222.619995  222.619995   \n",
      "\n",
      "               Volume  Dividends  Stock Splits                Ticker  \n",
      "Date                                                                  \n",
      "2024-07-24  167942900        0.0           0.0  TSLA_historical_data  \n",
      "2024-07-25  100636500        0.0           0.0  TSLA_historical_data  \n",
      "2024-07-26   94604100        0.0           0.0  TSLA_historical_data  \n",
      "2024-07-29  129201800        0.0           0.0  TSLA_historical_data  \n",
      "2024-07-30  100560300        0.0           0.0  TSLA_historical_data  \n",
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends',\n",
      "       'Stock Splits', 'Ticker'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocess and align data\n",
    "def preprocess_data(news_df, data_dict):\n",
    "    \"\"\"Clean and prepare data for analysis\"\"\"\n",
    "    # Convert news timestamp to datetime\n",
    "    news_df = news_df.copy()\n",
    "    news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "    news_df =news_df.dropna(subset=['date'])  # Remove rows where conversion failed\n",
    "    news_df['date'] =news_df['date'].dt.normalize().dt.tz_localize(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a combined stock DataFrame with ticker information\n",
    "    stock_dfs = []\n",
    "    for ticker, df in data_dict.items():\n",
    "        temp_df = df.copy()\n",
    "        temp_df['Ticker'] = ticker\n",
    "        stock_dfs.append(temp_df)\n",
    "    \n",
    "    combined_stocks=pd.concat(stock_dfs)\n",
    "    combined_stocks.index = pd.to_datetime(combined_stocks.index)\n",
    "\n",
    "    return news_df, combined_stocks\n",
    "# Call the function and store the result\n",
    "\n",
    "# Now preprocess and align the data\n",
    "clean_news_df, combined_stocks_df = preprocess_data(news_df, data_dict)\n",
    "\n",
    "# Check the results\n",
    "print(combined_stocks_df.tail())\n",
    "print(combined_stocks_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c6625ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "0   2020-06-05\n",
      "1   2020-06-03\n",
      "2   2020-05-26\n",
      "3   2020-05-22\n",
      "4   2020-05-22\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Parse datetime with timezone info\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "# Convert to timezone naive (optional)\n",
    "news_df['date'] = news_df['date'].dt.tz_localize(None)\n",
    "\n",
    "# Normalize to remove time (keep date only)\n",
    "news_df['date'] = news_df['date'].dt.normalize()\n",
    "\n",
    "print(news_df['date'].dtype)\n",
    "print(news_df['date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4558600",
   "metadata": {},
   "source": [
    "## ðŸ”„ Align News with Stock Trading Days\n",
    "\n",
    "This function aligns each news article with the correct stock trading day based on its timestamp.\n",
    "\n",
    "### ðŸ•’ Market Hours Assumption\n",
    "- **Market Open:** 9:30 AM  \n",
    "- **Market Close:** 4:00 PM  \n",
    "\n",
    "### ðŸ“Œ Logic:\n",
    "- News during trading hours â†’ same day (if valid trading day)\n",
    "- After market close â†’ next valid trading day\n",
    "- Before market open â†’ same day (if valid)\n",
    "\n",
    "### âœ… Returns:\n",
    "- `aligned_news`: News DataFrame with a new `trading_day` column.\n",
    "- Removes rows that couldn't be matched to a valid trading day.\n",
    "\n",
    "### ðŸ“‚ Requirements:\n",
    "- `news_df['date']` must be `datetime64[ns]`\n",
    "- `combined_stocks_df.index` must be `datetime64[ns]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ae41e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "        date stock trading_day  \n",
      "0 2020-06-05     A  2020-06-05  \n",
      "1 2020-06-03     A  2020-06-03  \n",
      "2 2020-05-26     A  2020-05-26  \n",
      "3 2020-05-22     A  2020-05-22  \n",
      "4 2020-05-22     A  2020-05-22  \n"
     ]
    }
   ],
   "source": [
    "from datetime import time\n",
    "import pandas as pd\n",
    "\n",
    "def align_news_with_stocks(news_df, combined_stocks_df):\n",
    "    \"\"\"Assign each news article to a valid trading day in the stock data.\"\"\"\n",
    "    \n",
    "    # Define market open and close times\n",
    "    MARKET_OPEN = time(9, 30)\n",
    "    MARKET_CLOSE = time(16, 0)\n",
    "\n",
    "    # Get unique trading days (dates only) from stock_dfs index\n",
    "    trading_days = combined_stocks_df.index.normalize().unique()\n",
    "\n",
    "    def assign_trading_day(date):\n",
    "        if pd.isnull(date):\n",
    "            return None\n",
    "\n",
    "        ts_time = date.time()\n",
    "        ts_date = date.date()\n",
    "        day = pd.Timestamp(ts_date)\n",
    "\n",
    "        # During market hours: assign to that day if valid\n",
    "        if MARKET_OPEN <= ts_time <= MARKET_CLOSE:\n",
    "            return day if day in trading_days else None\n",
    "\n",
    "        # After market close: assign to next trading day\n",
    "        elif ts_time > MARKET_CLOSE:\n",
    "            next_day = day + pd.Timedelta(days=1)\n",
    "            while next_day not in trading_days:\n",
    "                next_day += pd.Timedelta(days=1)\n",
    "            return next_day\n",
    "\n",
    "        # Before market open: assign to same day if valid\n",
    "        else:\n",
    "            return day if day in trading_days else None\n",
    "\n",
    "    # Apply the function to assign trading day for each news date\n",
    "    news_df['trading_day'] = news_df['date'].apply(assign_trading_day)\n",
    "\n",
    "    # Drop rows where trading_day assignment failed\n",
    "    aligned_news = news_df.dropna(subset=['trading_day'])\n",
    "\n",
    "    return aligned_news\n",
    "\n",
    "# Example usage:\n",
    "# Make sure news_df and stock_df are defined and have proper datetime indices\n",
    "# For example:\n",
    "# news_df['date'] should be datetime64[ns]\n",
    "# stock_df.index should be datetime64[ns]\n",
    "\n",
    "aligned_news = align_news_with_stocks(news_df, combined_stocks_df)\n",
    "\n",
    "# Print info to verify\n",
    "print(aligned_news.head())\n",
    "# aligned_news.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "962d16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News shape: (55987, 7)\n",
      "Stock shape: (45428, 9)\n",
      "Aligned News shape: (55230, 7)\n",
      "Missing in aligned_news:\n",
      "Unnamed: 0     0\n",
      "headline       0\n",
      "url            0\n",
      "publisher      0\n",
      "date           0\n",
      "stock          0\n",
      "trading_day    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"News shape:\", news_df.shape)\n",
    "print(\"Stock shape:\", combined_stocks_df.shape)\n",
    "print(\"Aligned News shape:\", aligned_news.shape)\n",
    "\n",
    "print(\"Missing in aligned_news:\")\n",
    "print(aligned_news.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aaa6bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day'],\n",
      "      dtype='object')\n",
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends',\n",
      "       'Stock Splits', 'Ticker'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(news_df.columns)\n",
    "print(combined_stocks_df.columns)\n",
    "print(aligned_news.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "28a801f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits', 'Ticker'],\n",
      "      dtype='object')\n",
      "   index       Date      Open      High       Low     Close  Adj Close  \\\n",
      "0      0 1980-12-12  0.128348  0.128906  0.128348  0.128348   0.098943   \n",
      "1      1 1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781   \n",
      "2      2 1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898   \n",
      "3      3 1980-12-17  0.115513  0.116071  0.115513  0.115513   0.089049   \n",
      "4      4 1980-12-18  0.118862  0.119420  0.118862  0.118862   0.091630   \n",
      "\n",
      "      Volume  Dividends  Stock Splits                Ticker  \n",
      "0  469033600        0.0           0.0  AAPL_historical_data  \n",
      "1  175884800        0.0           0.0  AAPL_historical_data  \n",
      "2  105728000        0.0           0.0  AAPL_historical_data  \n",
      "3   86441600        0.0           0.0  AAPL_historical_data  \n",
      "4   73449600        0.0           0.0  AAPL_historical_data  \n"
     ]
    }
   ],
   "source": [
    "combined_stocks_df = combined_stocks_df.drop(columns=['level_0', 'trading_day'], errors='ignore')\n",
    "print(combined_stocks_df.columns)\n",
    "print(combined_stocks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "26fd0528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day', 'neg', 'neu', 'pos', 'compound', 'index', 'Date', 'Open',\n",
      "       'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends',\n",
      "       'Stock Splits', 'Ticker'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0                                 headline  \\\n",
      "0           0  Stocks That Hit 52-Week Highs On Friday   \n",
      "1           0  Stocks That Hit 52-Week Highs On Friday   \n",
      "2           0  Stocks That Hit 52-Week Highs On Friday   \n",
      "3           0  Stocks That Hit 52-Week Highs On Friday   \n",
      "4           0  Stocks That Hit 52-Week Highs On Friday   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "3  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "4  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "\n",
      "        date stock trading_day  neg  neu  pos  ...       Date        Open  \\\n",
      "0 2020-06-05     A  2020-06-05  0.0  1.0  0.0  ... 2020-06-05   80.837502   \n",
      "1 2020-06-05     A  2020-06-05  0.0  1.0  0.0  ... 2020-06-05  122.225502   \n",
      "2 2020-06-05     A  2020-06-05  0.0  1.0  0.0  ... 2020-06-05   70.658501   \n",
      "3 2020-06-05     A  2020-06-05  0.0  1.0  0.0  ... 2020-06-05  226.710007   \n",
      "4 2020-06-05     A  2020-06-05  0.0  1.0  0.0  ... 2020-06-05  182.619995   \n",
      "\n",
      "         High         Low       Close   Adj Close     Volume  Dividends  \\\n",
      "0   82.937500   80.807503   82.875000   80.843407  137250400        0.0   \n",
      "1  124.432503  121.856499  124.150002  124.150002   66128000        0.0   \n",
      "2   72.252502   70.300003   71.919502   71.837753   34698000        0.0   \n",
      "3  231.350006  225.309998  230.770004  230.296753   16750400        0.0   \n",
      "4  187.729996  182.009995  187.199997  180.403564   39893600        0.0   \n",
      "\n",
      "   Stock Splits                Ticker  \n",
      "0           0.0  AAPL_historical_data  \n",
      "1           0.0  AMZN_historical_data  \n",
      "2           0.0  GOOG_historical_data  \n",
      "3           0.0  META_historical_data  \n",
      "4           0.0  MSFT_historical_data  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'trading_day' column in aligned news and stock_df['date'] exists\n",
    "aligned_news = news_df.dropna(subset=['trading_day']).copy()\n",
    "aligned_news['trading_day'] = pd.to_datetime(aligned_news['trading_day'])\n",
    "combined_stocks_df['Date'] = pd.to_datetime(combined_stocks_df['Date'])  # or from index if needed\n",
    "\n",
    "# Merge on 'trading_day'\n",
    "merged_df = pd.merge(aligned_news, combined_stocks_df, left_on='trading_day', right_on='Date', how='inner')\n",
    "print(merged_df.columns)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "76681f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "        date stock trading_day  neg    neu    pos  compound  neg    neu  \\\n",
      "0 2020-06-05     A  2020-06-05  0.0  1.000  0.000     0.000  0.0  1.000   \n",
      "1 2020-06-03     A  2020-06-03  0.0  1.000  0.000     0.000  0.0  1.000   \n",
      "2 2020-05-26     A  2020-05-26  0.0  1.000  0.000     0.000  0.0  1.000   \n",
      "3 2020-05-22     A  2020-05-22  0.0  1.000  0.000     0.000  0.0  1.000   \n",
      "4 2020-05-22     A  2020-05-22  0.0  0.833  0.167     0.296  0.0  0.833   \n",
      "\n",
      "     pos  compound  \n",
      "0  0.000     0.000  \n",
      "1  0.000     0.000  \n",
      "2  0.000     0.000  \n",
      "3  0.000     0.000  \n",
      "4  0.167     0.296  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon once\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "# Assuming your news dataframe has a 'headline' column (change it to your text column)\n",
    "news_df['sentiment'] = news_df['headline'].apply(analyze_sentiment)\n",
    "\n",
    "# This will create a new column with a dict of sentiment scores for each row:\n",
    "# Example of one row: {'neg': 0.0, 'neu': 0.3, 'pos': 0.7, 'compound': 0.8}\n",
    "\n",
    "# If you want to split these into separate columns:\n",
    "news_df = pd.concat([news_df.drop(['sentiment'], axis=1), \n",
    "                     news_df['sentiment'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# Now you have 'neg', 'neu', 'pos', and 'compound' as separate columns\n",
    "\n",
    "print(news_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1efe0ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trading_day       neg       neg       neu       neu       pos       pos  \\\n",
      "0   2011-04-27  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "1   2011-04-28  0.069000  0.069000  0.796000  0.796000  0.135000  0.135000   \n",
      "2   2011-04-29  0.000000  0.000000  0.820000  0.820000  0.180000  0.180000   \n",
      "3   2011-05-02  0.000000  0.000000  0.849889  0.849889  0.150111  0.150111   \n",
      "4   2011-05-03  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "5   2011-05-05  0.066667  0.066667  0.933333  0.933333  0.000000  0.000000   \n",
      "6   2011-05-06  0.000000  0.000000  0.944333  0.944333  0.055667  0.055667   \n",
      "7   2011-05-09  0.090667  0.090667  0.909333  0.909333  0.000000  0.000000   \n",
      "8   2011-05-11  0.000000  0.000000  0.814333  0.814333  0.185667  0.185667   \n",
      "9   2011-05-12  0.000000  0.000000  0.833000  0.833000  0.167000  0.167000   \n",
      "10  2011-05-13  0.000000  0.000000  0.811500  0.811500  0.188500  0.188500   \n",
      "11  2011-05-16  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "12  2011-05-17  0.145667  0.145667  0.794667  0.794667  0.059667  0.059667   \n",
      "13  2011-05-18  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "14  2011-05-19  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "\n",
      "    compound  compound  \n",
      "0   0.000000  0.000000  \n",
      "1   0.125000  0.125000  \n",
      "2   0.367550  0.367550  \n",
      "3   0.136444  0.136444  \n",
      "4   0.000000  0.000000  \n",
      "5  -0.042667 -0.042667  \n",
      "6   0.067433  0.067433  \n",
      "7  -0.222933 -0.222933  \n",
      "8   0.196667  0.196667  \n",
      "9   0.202300  0.202300  \n",
      "10  0.185475  0.185475  \n",
      "11  0.000000  0.000000  \n",
      "12 -0.045567 -0.045567  \n",
      "13  0.000000  0.000000  \n",
      "14  0.000000  0.000000  \n",
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day', 'neg', 'neu', 'pos', 'compound'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "daily_sentiment = news_df.groupby('trading_day')[['neg', 'neu', 'pos', 'compound']].mean().reset_index()\n",
    "print(daily_sentiment.head(15))\n",
    "print(aligned_news.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "287141bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                           headline  \\\n",
      "55225     1413787  Chinese Nano-Cap Momentum Stocks Sharply Highe...   \n",
      "55226     1413788  28 Stocks Moving In Wednesday's Pre-Market Ses...   \n",
      "55227     1413789  China Zenix Auto International Reports Q3 EPAD...   \n",
      "55228     1413790                   46 Biggest Movers From Yesterday   \n",
      "55229     1413791      42 Stocks Moving In Tuesday's Mid-Day Session   \n",
      "\n",
      "                                                     url      publisher  \\\n",
      "55225  https://www.benzinga.com/movers/18/01/10994518...  Paul Quintaro   \n",
      "55226  https://www.benzinga.com/news/17/12/10878295/2...     Lisa Levin   \n",
      "55227  https://www.benzinga.com/news/earnings/17/12/1...  Paul Quintaro   \n",
      "55228  https://www.benzinga.com/news/17/11/10788120/4...     Lisa Levin   \n",
      "55229  https://www.benzinga.com/news/17/11/10782008/4...     Lisa Levin   \n",
      "\n",
      "            date stock trading_day       neg       neg       neu       neu  \\\n",
      "55225 2018-01-05    ZX  2018-01-05  0.037077  0.037077  0.927615  0.927615   \n",
      "55226 2017-12-06    ZX  2017-12-06  0.017167  0.017167  0.939583  0.939583   \n",
      "55227 2017-12-06    ZX  2017-12-06  0.017167  0.017167  0.939583  0.939583   \n",
      "55228 2017-11-15    ZX  2017-11-15  0.005250  0.005250  0.826250  0.826250   \n",
      "55229 2017-11-14    ZX  2017-11-14  0.026467  0.026467  0.910533  0.910533   \n",
      "\n",
      "            pos       pos  compound  compound  \n",
      "55225  0.035308  0.035308  0.026592  0.026592  \n",
      "55226  0.043250  0.043250  0.067492  0.067492  \n",
      "55227  0.043250  0.043250  0.067492  0.067492  \n",
      "55228  0.168500  0.168500  0.252625  0.252625  \n",
      "55229  0.062933  0.062933  0.084040  0.084040  \n"
     ]
    }
   ],
   "source": [
    "# Make sure 'trading_day' column is datetime type and normalized (no time part)\n",
    "aligned_news = aligned_news.drop(columns=['neg', 'neu', 'pos', 'compound'], errors='ignore')\n",
    "aligned_news['trading_day'] = pd.to_datetime(aligned_news['trading_day']).dt.normalize()\n",
    "\n",
    "# Also normalize the daily_sentiment trading_day column to match exactly\n",
    "daily_sentiment['trading_day'] = pd.to_datetime(daily_sentiment['trading_day']).dt.normalize()\n",
    "\n",
    "# Merge stock data with daily sentiment scores\n",
    "merged_df = pd.merge(aligned_news, daily_sentiment, on='trading_day', how='left')\n",
    "\n",
    "print(merged_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "77f3208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day', 'neg', 'neu', 'pos', 'compound'],\n",
      "      dtype='object')\n",
      "Index(['index', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits', 'Ticker'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(aligned_news.columns)\n",
    "print(combined_stocks_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "470c0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       Date      Open      High       Low     Close  Adj Close  \\\n",
      "0      0 1980-12-12  0.128348  0.128906  0.128348  0.128348   0.098943   \n",
      "1      1 1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781   \n",
      "2      2 1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898   \n",
      "3      3 1980-12-17  0.115513  0.116071  0.115513  0.115513   0.089049   \n",
      "4      4 1980-12-18  0.118862  0.119420  0.118862  0.118862   0.091630   \n",
      "\n",
      "      Volume  Dividends  Stock Splits                Ticker  \n",
      "0  469033600        0.0           0.0  AAPL_historical_data  \n",
      "1  175884800        0.0           0.0  AAPL_historical_data  \n",
      "2  105728000        0.0           0.0  AAPL_historical_data  \n",
      "3   86441600        0.0           0.0  AAPL_historical_data  \n",
      "4   73449600        0.0           0.0  AAPL_historical_data  \n"
     ]
    }
   ],
   "source": [
    "print(combined_stocks_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8b086315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock',\n",
      "       'trading_day', 'neg', 'neg', 'neu', 'neu', 'pos', 'pos', 'compound',\n",
      "       'compound'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f603cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       Date      Open      High       Low     Close  Adj Close  \\\n",
      "1      1 1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781   \n",
      "2      2 1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898   \n",
      "3      3 1980-12-17  0.115513  0.116071  0.115513  0.115513   0.089049   \n",
      "4      4 1980-12-18  0.118862  0.119420  0.118862  0.118862   0.091630   \n",
      "5      5 1980-12-19  0.126116  0.126674  0.126116  0.126116   0.097223   \n",
      "\n",
      "      Volume  Dividends  Stock Splits                Ticker  stock_return  \n",
      "1  175884800        0.0           0.0  AAPL_historical_data     -0.052171  \n",
      "2  105728000        0.0           0.0  AAPL_historical_data     -0.073398  \n",
      "3   86441600        0.0           0.0  AAPL_historical_data      0.024751  \n",
      "4   73449600        0.0           0.0  AAPL_historical_data      0.028992  \n",
      "5   48630400        0.0           0.0  AAPL_historical_data      0.061029  \n"
     ]
    }
   ],
   "source": [
    "# Calculate daily returns as percentage change of the 'Close' price\n",
    "combined_stocks_df['stock_return'] = combined_stocks_df['Close'].pct_change()\n",
    "\n",
    "# Drop the first row which will have NaN return\n",
    "combined_stocks_df = combined_stocks_df.dropna(subset=['stock_return'])\n",
    "print(combined_stocks_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d28cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['neg', 'neu', 'pos', 'compound'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate correlation matrix for sentiment scores and stock returns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m merged_new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(aligned_news, combined_stocks_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_return\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[0;32m      3\u001b[0m                      left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrading_day\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m correlation \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_new_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompound\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(correlation)\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['neg', 'neu', 'pos', 'compound'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Calculate correlation matrix for sentiment scores and stock returns\n",
    "correlation = merged_df[['neg', 'neu', 'pos', 'compound', 'stock_return']].corr()\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)\n",
    "\n",
    "# Optional: Correlation of stock_return with each sentiment score individually\n",
    "for col in ['neg', 'neu', 'pos', 'compound']:\n",
    "    corr_value = merged_df['stock_return'].corr(merged_df[col])\n",
    "    print(f\"Correlation between stock returns and {col} sentiment: {corr_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040db10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     corr_coef, p_value \u001b[38;5;241m=\u001b[39m \u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sentiment: Pearson r = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorr_coef\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p-value = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4794\u001b[0m, in \u001b[0;36mpearsonr\u001b[1;34m(x, y, alternative, method)\u001b[0m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;66;03m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \u001b[38;5;66;03m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001b[39;00m\n\u001b[0;32m   4792\u001b[0m \u001b[38;5;66;03m# [-5e210, 5e210, 3e200, -3e200]\u001b[39;00m\n\u001b[0;32m   4793\u001b[0m normxm \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(xm)\n\u001b[1;32m-> 4794\u001b[0m normym \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mym\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4796\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-13\u001b[39m\n\u001b[0;32m   4797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normxm \u001b[38;5;241m<\u001b[39m threshold\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(xmean) \u001b[38;5;129;01mor\u001b[39;00m normym \u001b[38;5;241m<\u001b[39m threshold\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(ymean):\n\u001b[0;32m   4798\u001b[0m     \u001b[38;5;66;03m# If all the values in x (likewise y) are very close to the mean,\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m     \u001b[38;5;66;03m# the loss of precision that occurs in the subtraction xm = x - xmean\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m     \u001b[38;5;66;03m# might result in large errors in r.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\scipy\\linalg\\_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[1;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_chkfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[1;32mc:\\Users\\Student\\finance_week1\\venv\\lib\\site-packages\\numpy\\lib\\function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "for col in ['neg', 'neu', 'pos', 'compound']:\n",
    "    corr_coef, p_value = pearsonr(merged_df['stock_return'], merged_df[col])\n",
    "    print(f\"{col} sentiment: Pearson r = {corr_coef:.4f}, p-value = {p_value:.4g}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finance_week1)",
   "language": "python",
   "name": "finance_week1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
